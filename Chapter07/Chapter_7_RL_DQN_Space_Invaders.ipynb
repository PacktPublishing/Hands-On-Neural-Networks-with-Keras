{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    \n",
    "    def process_observation(self, observation):\n",
    "        \n",
    "        assert observation.ndim == 3                        # Assert dimension (height, width, channel)\n",
    "        \n",
    "        img = Image.fromarray(observation)                  # Retrieve image from array\n",
    "        \n",
    "        img = img.resize(INPUT_SHAPE).convert('L')          # Resize and convert to grayscale\n",
    "        \n",
    "        processed_observation = np.array(img)               # Convert back to array\n",
    "        \n",
    "        assert processed_observation.shape == INPUT_SHAPE   # Assert input shape\n",
    "        \n",
    "        return processed_observation.astype('uint8')        # Save processed observation in experience memory (8bit)\n",
    "    \n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "\n",
    "        processed_batch = batch.astype('float32') / 255.    #Convert the batches of images to float32 datatype\n",
    "        \n",
    "        return processed_batch\n",
    "    \n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \n",
    "        return np.clip(reward, -1., 1.)                     # Clip reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize space invaders environment from OpenAi gym (Atari dependency required)\n",
    "\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define input shape\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "\n",
    "# Build Conv2D model\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear')) # Last layer: no. of neurons corresponds to action space \n",
    "                                                  # Linear activation\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sequential memory for experience replay\n",
    "\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize epsilon greedy exploration policy ( Mihn et al., 2015)\n",
    "# Try Boltzmann Q policy\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps',\n",
    "                              value_max=1.,\n",
    "                              value_min=.1,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the atari_processor() class\n",
    "\n",
    "processor = AtariProcessor()\n",
    "\n",
    "# Initialize the DQN agent \n",
    "\n",
    "\n",
    "dqn = DQNAgent(model=model,                 #Compiled neural network model\n",
    "               nb_actions=nb_actions,       #Action space\n",
    "               policy=policy,               #Policy chosen (Try Boltzman Q policy)\n",
    "               memory=memory,               #Replay memory (Try Episode Parameter memory)\n",
    "               processor=processor,         #Atari processor class\n",
    "               nb_steps_warmup=50000,       #Warmup steps to ignore initially (due to random initial weights)\n",
    "               gamma=.99,                   #Discount factor\n",
    "               train_interval=4,            #Training intervals\n",
    "               delta_clip=1.,               #Reward clipping\n",
    "              )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double DQN\n",
    "\n",
    "double_dqn = DQNAgent(model=model,\n",
    "               nb_actions=nb_actions,\n",
    "               policy=policy,\n",
    "               memory=memory,\n",
    "               processor=processor,\n",
    "               nb_steps_warmup=50000,\n",
    "               gamma=.99, \n",
    "               target_model_update=1e-2,\n",
    "               train_interval=4,\n",
    "               delta_clip=1.,\n",
    "               enable_double_dqn=True,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dueling DQN\n",
    "\n",
    "dueling_dqn = DQNAgent(model=model,\n",
    "               nb_actions=nb_actions,\n",
    "               policy=policy,\n",
    "               memory=memory,\n",
    "               processor=processor,\n",
    "               nb_steps_warmup=50000,\n",
    "               gamma=.99, \n",
    "               target_model_update=10000,\n",
    "               train_interval=4,\n",
    "               delta_clip=1.,\n",
    "               enable_dueling_network=True,\n",
    "               dueling_type='avg'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile DQN agent\n",
    "\n",
    "dqn.compile(optimizer=Adam(lr=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1750000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 2697/10000 [=======>......................] - ETA: 26s - reward: 0.0126"
     ]
    }
   ],
   "source": [
    "# Initiate training\n",
    "\n",
    "dqn.fit(env, nb_steps=1750000)   #visualize=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 3.000, steps: 654\n",
      "Episode 2: reward: 11.000, steps: 807\n",
      "Episode 3: reward: 8.000, steps: 812\n",
      "Episode 4: reward: 3.000, steps: 475\n",
      "Episode 5: reward: 4.000, steps: 625\n",
      "Episode 6: reward: 9.000, steps: 688\n",
      "Episode 7: reward: 5.000, steps: 652\n",
      "Episode 8: reward: 12.000, steps: 826\n",
      "Episode 9: reward: 2.000, steps: 632\n",
      "Episode 10: reward: 3.000, steps: 643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24280aadc50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test agent\n",
    "\n",
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  || DQN algorithm ||\n",
    "\n",
    "\n",
    "\n",
    "    initialize replay memory\n",
    "    initialize Q-Value function with random weights\n",
    "    sample initial state from environment\n",
    "\n",
    "    Keep repeating:\n",
    "\n",
    "        choose an action to perform:\n",
    "\n",
    "            with probability ε select a random action\n",
    "            otherwise select action with argmax a Q(s, a')\n",
    "\n",
    "        execute chosen action\n",
    "        collect reward and next state\n",
    "        save experience <s, a, r, s'> in replay memory\n",
    "\n",
    "        sample random transitions <s, a, r, s'> from replay memory\n",
    "        compute target variable for each mini-batch transition:\n",
    "\n",
    "            if s' is terminal state then target = r\n",
    "            otherwise t = r + γ max a'Q(s', a')\n",
    "\n",
    "        train the network with loss (target - Q(s,a)`^2)\n",
    "\n",
    "        s = s'\n",
    "\n",
    "    until done\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
